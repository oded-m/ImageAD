{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import random\n",
    "from pycaret.classification import *\n",
    "from pycaret.datasets import get_data\n",
    "\n",
    "from create_datasets import create_train_dataset_supervised, \\\n",
    "    create_test_dataset_balanced, \\\n",
    "    create_test_dataset_unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "WORK_DIR = os.path.dirname('.')\n",
    "FM_DIR = os.path.join(WORK_DIR, 'feature_maps')\n",
    "BENCHMARK_DIR = os.path.join(WORK_DIR, 'benchmark')\n",
    "DATA_DIR = os.path.join(WORK_DIR, 'data')\n",
    "OUTPUTS_DIR = os.path.join(WORK_DIR, 'outputs')\n",
    "NUM_OF_DATA_SETS = 30\n",
    "\n",
    "if not os.path.isdir(join(OUTPUTS_DIR, 'results_train_sup')):\n",
    "    os.mkdir(join(OUTPUTS_DIR, 'results_train_sup'))\n",
    "if not os.path.isdir(join(OUTPUTS_DIR, 'results_test_sup_balanced')):\n",
    "    os.mkdir(join(OUTPUTS_DIR, 'results_test_sup_balanced'))\n",
    "if not os.path.isdir(join(OUTPUTS_DIR, 'results_test_sup_unbalanced')):\n",
    "    os.mkdir(join(OUTPUTS_DIR, 'results_test_sup_unbalanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "df_results_test_sup_balanced = None\n",
    "for data_set_idx in range(NUM_OF_DATA_SETS):\n",
    "    sup_train_balanced_results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'AUC', 'Recall', 'Prec.', 'F1', 'Kappa', 'MCC'])\n",
    "    X_train_sup, y_train_sup, sizes_train = create_train_dataset_supervised(data_set_idx)\n",
    "    X_test_b, y_test_b, sizes_balanced = create_test_dataset_balanced(data_set_idx)\n",
    "    X_test_ub, y_test_ub, sizes_unbalanced = create_test_dataset_unbalanced(data_set_idx)\n",
    "\n",
    "    data_train_df = pd.DataFrame(X_train_sup)\n",
    "    data_test_b_df = pd.DataFrame(X_test_b)\n",
    "    data_test_ub_df = pd.DataFrame(X_test_ub)\n",
    "    \n",
    "    data_train_df['Labels'] = y_train_sup\n",
    "    data_test_b_df['Labels'] = y_test_b\n",
    "    data_test_ub_df['Labels'] = y_test_ub\n",
    "    \n",
    "    \n",
    "    # TRAIN\n",
    "    setup(data_train_df, target='Labels', session_id=42, n_jobs=25, use_gpu=True)\n",
    "    compare_models(fold=5, exclude=['gbc', 'ada', 'lightgbm', 'dt'], n_select=12)\n",
    "    sup_train_balanced_results_df = pull()\n",
    "    sup_train_balanced_results_df['n_normal'] = sizes_train[0]\n",
    "    sup_train_balanced_results_df['n_anomaly'] = sizes_train[1]\n",
    "    sup_train_balanced_results_df.to_csv(join(OUTPUTS_DIR, 'results_train_sup', f'id_{data_set_idx}.csv'))\n",
    "    \n",
    "    # TEST BALANCED\n",
    "    setup(data_train_df, test_data=data_test_b_df, index=False, target='Labels', session_id=42, n_jobs=25, use_gpu=True)\n",
    "    list_of_best_models_balanced = compare_models(fold=5, exclude=['gbc', 'ada', 'lightgbm', 'dt'], n_select=12)\n",
    "    \n",
    "    df_results_test_sup_balanced = pd.DataFrame()\n",
    "    for idx, model in enumerate(list_of_best_models_balanced):\n",
    "        predict_model(list_of_best_models_balanced[idx])\n",
    "        df_results_test_sup_balanced = df_results_test_sup_balanced.append(pull())\n",
    "\n",
    "    df_results_test_sup_balanced['n_normal'] = sizes_balanced[0]\n",
    "    df_results_test_sup_balanced['n_anomaly'] = sizes_balanced[1]\n",
    "    df_results_test_sup_balanced.to_csv(join(OUTPUTS_DIR, 'results_test_sup_balanced', f'id_{data_set_idx}.csv'))\n",
    "    \n",
    "    \n",
    "    # TEST UNBALANCED\n",
    "    setup(data_train_df, test_data=data_test_ub_df, index=False, target='Labels', session_id=42, n_jobs=25, use_gpu=True)\n",
    "    list_of_best_models_unbalanced = compare_models(fold=5, exclude=['gbc', 'ada', 'lightgbm', 'dt'], n_select=12)\n",
    "    \n",
    "    df_results_test_sup_unbalanced = pd.DataFrame()\n",
    "    for idx, model in enumerate(list_of_best_models_unbalanced):\n",
    "        predict_model(list_of_best_models_unbalanced[idx])\n",
    "        df_results_test_sup_unbalanced = df_results_test_sup_unbalanced.append(pull())\n",
    "\n",
    "    df_results_test_sup_unbalanced['n_normal'] = sizes_unbalanced[0]\n",
    "    df_results_test_sup_unbalanced['n_anomaly'] = sizes_unbalanced[1]\n",
    "    df_results_test_sup_unbalanced.to_csv(join(OUTPUTS_DIR, 'results_test_sup_unbalanced', f'id_{data_set_idx}.csv'))\n",
    "    \n",
    "    counter += 1\n",
    "        \n",
    "    print(f'Finnish: {counter} / {NUM_OF_DATA_SETS}')\n",
    "    \n",
    "print(f'Finnished all: {counter} / {NUM_OF_DATA_SETS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258bcbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['bAccAvg', 'bAccStd', 'bAccMax', 'bAccMin', 'bAccMaxName', 'bAccMinName',\n",
    "                 'bAUCAvg', 'bAUCStd', 'bAUCMax', 'bAUCMin', 'bAUCMaxName', 'bAUCMinName',\n",
    "                 'bF1Avg', 'bF1Std', 'bF1Max', 'bF1Min', 'bF1MaxName', 'bF1MinName',\n",
    "                 'ubAccAvg', 'ubAccStd', 'ubAccMax', 'ubAccMin', 'ubAccMaxName', 'ubAccMinName',\n",
    "                 'ubAUCAvg', 'ubAUCStd', 'ubAUCMax', 'ubAUCMin', 'ubAUCMaxName', 'ubAUCMinName',\n",
    "                 'ubF1Avg', 'ubF1Std', 'ubF1Max', 'ubF1Min', 'ubF1MaxName', 'ubF1MinName',\n",
    "                 'NTrSOrig', 'ATrSOrig', 'NTrSBalanced', 'ATrSBalanced', 'NTsSOrig', 'ATsSOrig',\n",
    "                 'NTsSUnBalanced', 'ATsSUnBalanced'\n",
    "                 ]\n",
    "sup_results_df = pd.DataFrame(columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_files = glob.glob(os.path.join(OUTPUTS_DIR, 'results_train_sup', '*.csv'))\n",
    "all_test_files_sup_balanced = glob.glob(os.path.join(OUTPUTS_DIR, 'results_test_sup_balanced', '*.csv'))\n",
    "all_test_files_sup_unbalanced = glob.glob(os.path.join(OUTPUTS_DIR, 'results_test_sup_unbalanced', '*.csv'))\n",
    "results_train_sup_balanced_df = pd.concat((pd.read_csv(f) for f in all_train_files), ignore_index=True)\n",
    "results_test_files_sup_balanced_df = pd.concat((pd.read_csv(f) for f in all_test_files_sup_balanced), ignore_index=True)\n",
    "results_test_files_sup_unbalanced_df = pd.concat((pd.read_csv(f) for f in all_test_files_sup_unbalanced), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbbeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['bAccAvg', 'bAccStd', 'bAccMax', 'bAccMin', 'bAccMaxName', 'bAccMinName',\n",
    "                 'bAUCAvg', 'bAUCStd', 'bAUCMax', 'bAUCMin', 'bAUCMaxName', 'bAUCMinName',\n",
    "                 'bF1Avg', 'bF1Std', 'bF1Max', 'bF1Min', 'bF1MaxName', 'bF1MinName']\n",
    "\n",
    "def create_dataset_balanced_summary(df: pd.DataFrame()):\n",
    "    result = {}\n",
    "    result['bAccAvg'] = df['Accuracy'].mean()\n",
    "    result['bAccStd'] = df['Accuracy'].std()\n",
    "    result['bAccMax'] = df['Accuracy'].max()\n",
    "    result['bAccMin'] = df['Accuracy'].min()\n",
    "    result['bAccMaxName'] = df.iloc[df['Accuracy'].idxmax(axis=0)]['Model']\n",
    "    result['bAccMinName'] = df.iloc[df['Accuracy'].idxmin(axis=0)]['Model']\n",
    "    result['bAUCAvg'] = df['AUC'].mean()\n",
    "    result['bAUCStd'] = df['AUC'].std()\n",
    "    result['bAUCMax'] = df['AUC'].max()\n",
    "    result['bAUCMin'] = df['AUC'].min()\n",
    "    result['bAUCMaxName'] = df.iloc[df['AUC'].idxmax(axis=0)]['Model']\n",
    "    result['bAUCMinName'] = df.iloc[df['AUC'].idxmin(axis=0)]['Model']\n",
    "    result['bF1Avg'] = df['F1'].mean()\n",
    "    result['bF1Std'] = df['F1'].std()\n",
    "    result['bF1Max'] = df['F1'].max()\n",
    "    result['bF1Min'] = df['F1'].min()\n",
    "    result['bF1MaxName'] = df.iloc[df['F1'].idxmax(axis=0)]['Model']\n",
    "    result['bF1MinName'] = df.iloc[df['F1'].idxmin(axis=0)]['Model']\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def create_dataset_unbalanced_summary(df: pd.DataFrame()):\n",
    "    result = {}\n",
    "    result['ubAccAvg'] = df['Accuracy'].mean()\n",
    "    result['ubAccStd'] = df['Accuracy'].std()\n",
    "    result['ubAccMax'] = df['Accuracy'].max()\n",
    "    result['ubAccMin'] = df['Accuracy'].min()\n",
    "    result['ubAccMaxName'] = df.iloc[df['Accuracy'].idxmax(axis=0)]['Model']\n",
    "    result['ubAccMinName'] = df.iloc[df['Accuracy'].idxmin(axis=0)]['Model']\n",
    "    result['ubAUCAvg'] = df['AUC'].mean()\n",
    "    result['ubAUCStd'] = df['AUC'].std()\n",
    "    result['ubAUCMax'] = df['AUC'].max()\n",
    "    result['ubAUCMin'] = df['AUC'].min()\n",
    "    result['ubAUCMaxName'] = df.iloc[df['AUC'].idxmax(axis=0)]['Model']\n",
    "    result['ubAUCMinName'] = df.iloc[df['AUC'].idxmin(axis=0)]['Model']\n",
    "    result['ubF1Avg'] = df['F1'].mean()\n",
    "    result['ubF1Std'] = df['F1'].std()\n",
    "    result['ubF1Max'] = df['F1'].max()\n",
    "    result['ubF1Min'] = df['F1'].min()\n",
    "    result['ubF1MaxName'] = df.iloc[df['F1'].idxmax(axis=0)]['Model']\n",
    "    result['ubF1MinName'] = df.iloc[df['F1'].idxmin(axis=0)]['Model']\n",
    "\n",
    "    return result\n",
    "\n",
    "results_sup_balanced_test = pd.DataFrame()\n",
    "for df in [pd.read_csv(f) for f in all_test_files_sup_balanced]:\n",
    "    del df['Unnamed: 0']\n",
    "    result = create_dataset_summary(df)\n",
    "    result['n_normal'] = df['n_normal'][0]\n",
    "    result['n_anomaly'] = df['n_anomaly'][0]\n",
    "    results_sup_balanced_test = results.append(result, ignore_index=True)\n",
    "\n",
    "\n",
    "results_sup_unbalanced_test = pd.DataFrame()\n",
    "for df in [pd.read_csv(f) for f in all_test_files_sup_unbalanced]:\n",
    "    del df['Unnamed: 0']\n",
    "    result = create_dataset_unbalanced_summary(df)\n",
    "    result['n_normal'] = df['n_normal'][0]\n",
    "    result['n_anomaly'] = df['n_anomaly'][0]\n",
    "    results_sup_unbalanced_test = results_sup_unbalanced_test.append(result, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf13cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sup_balanced_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb66952",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sup_unbalanced_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
